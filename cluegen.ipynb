{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-16T12:48:18.395404Z",
     "start_time": "2023-08-16T12:48:13.757367Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import spacy\n",
    "import cluegen\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "#import nltk\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(os.path.join(path, 'google-10000-english-no-swears.txt'), 'r') as f:\n",
    "#     for line in f:\n",
    "#         token = line.split('\\t')[0]\n",
    "#         lemma = nlp(token)\n",
    "#         # filter out stop words and only keep nouns, verbs and adjectives\n",
    "#         if lemma.has_vector and not lemma[0].is_stop and lemma[0].pos_ in ['NOUN', 'VERB', 'ADJ']:\n",
    "#             all_clues.append(lemma)\n",
    "#             if lemma[0].pos_ == 'NOUN':\n",
    "#                 num_nouns += 1\n",
    "#             elif lemma[0].pos_ == 'VERB':\n",
    "#                 num_verbs += 1\n",
    "#             elif lemma[0].pos_ == 'ADJ':\n",
    "#                 num_adjs += 1\n",
    "\n",
    "# print(len(all_clues))\n",
    "# print(num_nouns, 'nouns')\n",
    "# print(num_verbs, 'verbs')\n",
    "# print(num_adjs, 'adjectives')\n",
    "\n",
    "\n",
    "# open filtered clues\n",
    "all_clues = []\n",
    "lemmas = set()\n",
    "with open('filtered_clues.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        token = line.strip()\n",
    "        lemma = nlp(token)\n",
    "        if lemma.text not in lemmas:\n",
    "            lemmas.add(lemma.text)\n",
    "            all_clues.append(lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63202"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # remove any words that are not nouns, verbs, or adjectives\n",
    "# all_clues = [w for w in all_clues if w[0].pos_ in ['NOUN', 'VERB', 'ADJ']]\n",
    "\n",
    "# # save filtered clues to a new text file\n",
    "# with open('filtered_clues.txt', 'w') as f:\n",
    "#     for clue in all_clues:\n",
    "#         f.write(clue.text + '\\n')\n",
    "\n",
    "# # words from https://github.com/first20hours/google-10000-english\n",
    "# import pickle\n",
    "# with open('filtered_clues.pkl', 'wb') as f:\n",
    "#     pickle.dump(all_clues, f)\n",
    "# len(all_clues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2477\n",
      "1293 nouns\n",
      "796 verbs\n",
      "388 adjectives\n"
     ]
    }
   ],
   "source": [
    "path = './'\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# all_clues = []\n",
    "# num_nouns = 0\n",
    "# num_verbs = 0\n",
    "# num_adjs = 0\n",
    "# with open(os.path.join(path, 'top_words.txt'), 'r') as f:\n",
    "#     for line in f:\n",
    "#         token = line.strip()\n",
    "#         lemma = nlp(token)\n",
    "#         # filter out stop words and only keep nouns, verbs and adjectives\n",
    "#         if lemma.has_vector and not lemma[0].is_stop and lemma[0].pos_ in ['NOUN', 'VERB', 'ADJ']:\n",
    "#             all_clues.append(lemma)\n",
    "#             if lemma[0].pos_ == 'NOUN':\n",
    "#                 num_nouns += 1\n",
    "#             elif lemma[0].pos_ == 'VERB':\n",
    "#                 num_verbs += 1\n",
    "#             elif lemma[0].pos_ == 'ADJ':\n",
    "#                 num_adjs += 1\n",
    "# print(len(all_clues))\n",
    "# print(num_nouns, 'nouns')\n",
    "# print(num_verbs, 'verbs')\n",
    "# print(num_adjs, 'adjectives')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "clue_generator = cluegen.ClueGenerator(path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['piano', 'guitar'] --> ['song' 'band' 'bass']\n",
      "['moon', 'sky'] --> ['sun' 'sea' 'air']\n",
      "['sun', 'moon', 'sky'] --> ['sea' 'air' 'eye']\n",
      "['tree', 'flower'] --> ['sun' 'oil' 'wood']\n",
      "['water', 'air'] --> ['gas' 'oil' 'sea']\n",
      "['ring', 'table'] --> ['ions' 'low' 'use']\n",
      "['computer', 'phone'] --> ['app' 'web' 'user']\n",
      "['car', 'house'] --> ['bus' 'room' 'homes']\n",
      "['apple', 'tree', 'flower'] --> ['oil' 'tea' 'sun']\n",
      "['fire', 'water', 'air'] --> ['gas' 'oil' 'sea']\n",
      "['dog', 'car', 'house'] --> ['pet' 'room' 'bus']\n",
      "['book', 'pen'] --> ['ink' 'art' 'web']\n",
      "['Earth', 'cloud'] --> ['sky' 'sun' 'air']\n"
     ]
    }
   ],
   "source": [
    "# make a list of 25 words\n",
    "my_words = ['Africa', 'ring', 'dog', 'Earth', 'piano', \n",
    "         'apple', 'chair', 'table', 'book', 'pen', \n",
    "         'computer', 'phone', 'guitar', 'car', 'house', \n",
    "         'tree', 'flower', 'water', 'air', 'fire', \n",
    "         'sun', 'moon', 'star', 'sky', 'cloud']\n",
    "your_words = ['ocean', 'mountain', 'river', 'lake', 'road',\n",
    "               'bridge', 'building', 'city', 'country', 'continent',\n",
    "               'planet', 'universe', 'galaxy', 'solar', 'fish']\n",
    "\n",
    "clusters, clues = clue_generator.generate(my_words)\n",
    "for cluster, clue in zip(clusters, clues):\n",
    "    print(cluster, '-->', clue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4 [piano, guitar]\n",
      "0.53 [moon, sky]\n",
      "0.6 [sun, moon, sky]\n",
      "0.69 [tree, flower]\n",
      "0.75 [water, air]\n",
      "0.88 [ring, table]\n",
      "0.9 [computer, phone]\n",
      "0.94 [car, house]\n",
      "0.99 [apple, tree, flower]\n",
      "1.03 [fire, water, air]\n",
      "1.17 [dog, car, house]\n",
      "1.19 [book, pen]\n",
      "1.2 [Earth, cloud]\n",
      "1.3 [chair, dog, car, house]\n",
      "1.33 [ring, table, apple, tree, flower]\n",
      "1.41 [sun, moon, sky, Earth, cloud]\n",
      "1.46 [Africa, star]\n",
      "1.49 [computer, phone, book, pen]\n"
     ]
    }
   ],
   "source": [
    "clusters = clue_generator.cluster_words(words)\n",
    "for cluster in clusters:\n",
    "    print(np.round(cluster.distance, 2), cluster.get_lemmas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_dist(v1, v2):\n",
    "    return 1 - (np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster\n",
      "[piano, guitar]\n",
      "0.17 violin \n",
      "0.2 saxophone [0.86 single-reed_instrument] \n",
      "0.22 harpsichord [0.41 clavier] [0.66 spinet] [0.92 virginal] \n",
      "0.23 banjo \n",
      "0.24 trombone [0.57 brass] [0.91 sackbut] \n",
      "0.24 mandolin [0.55 mandola] \n",
      "0.24 bass [0.73 pitch] \n",
      "0.26 drumset 0.26 solos [1.03 activity] \n",
      "0.26 banjos \n"
     ]
    }
   ],
   "source": [
    "# TODO: improve the clue generation process\n",
    "\n",
    "# 1) make a better list of possible clue words\n",
    "#    (e.g. only nouns and some verbs and adjectives)\n",
    "# 2) get the top few clues to a word\n",
    "# 3) then, check those words hypernyms for similarity as well\n",
    "#    return the most similar of the possibilities\n",
    "\n",
    "cluster = clusters[0]\n",
    "cluster_words = [c.text.lower() for c in cluster.get_lemmas()]\n",
    "\n",
    "print('Cluster')\n",
    "print(cluster.get_lemmas())\n",
    "\n",
    "def no_word_overlap(word, words):\n",
    "    # make sure the word does not equal,\n",
    "    # or is not a substring of any of the words, and vice versa\n",
    "    return (word not in words) and (not any([word in w for w in words]) and (not any([w in word for w in words])))\n",
    "\n",
    "all_clues_less_cluster = []\n",
    "for clue_lemma in all_clues:\n",
    "    clue_word = clue_lemma.text.lower()\n",
    "    if no_word_overlap(clue_word, cluster_words):\n",
    "        all_clues_less_cluster.append(clue_lemma)\n",
    "\n",
    "# get the average vector for the cluster\n",
    "cluster_avg = np.mean([w.vector for w in cluster.get_lemmas()], axis=0)\n",
    "# get the distance from each word to the cluster average using cosine distance\n",
    "#distances = [np.linalg.norm(w.vector - cluster_avg) for w in all_clues_less_cluster]\n",
    "distances = [cosine_dist(w.vector, cluster_avg) for w in all_clues_less_cluster]\n",
    "# get the index of the word with the smallest distance\n",
    "clue_index = np.argmin(distances)\n",
    "# return the top 10 words with the smallest distance\n",
    "clues = [(all_clues_less_cluster[i], distances[i]) for i in np.argsort(distances)[:10]]\n",
    "for clue, dist, in clues:\n",
    "    print(np.round(dist, 2), clue, end=' ')\n",
    "    clue_wn = wordnet.synsets(clue.text)\n",
    "    if len(clue_wn) > 0:\n",
    "        clue_wn = clue_wn[0] # get the most common synset\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    for hyper in clue_wn.hypernyms():\n",
    "        hyper_word = hyper.name().split('.')[0]\n",
    "        hyper_lemma = nlp(hyper_word)\n",
    "        if hyper_lemma.has_vector:\n",
    "            hyper_dist = cosine_dist(nlp(hyper_word).vector, cluster_avg)\n",
    "            print('['+str(np.round(hyper_dist, 2)), hyper_word, end='] ')\n",
    "    for hypo in clue_wn.hyponyms():\n",
    "        hypo_word = hypo.name().split('.')[0]\n",
    "        hypo_lemma = nlp(hypo_word)\n",
    "        if hypo_lemma.has_vector:\n",
    "            hypo_dist = cosine_dist(nlp(hypo_word).vector, cluster_avg)\n",
    "            print('['+str(np.round(hypo_dist, 2)), hypo_word, end='] ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4 [piano, guitar]\n",
      "0.53 [moon, sky]\n",
      "0.6 [sun, moon, sky]\n",
      "0.69 [tree, flower]\n",
      "0.75 [water, air]\n",
      "0.88 [ring, table]\n",
      "0.9 [computer, phone]\n",
      "0.94 [car, house]\n",
      "0.99 [apple, tree, flower]\n",
      "1.03 [fire, water, air]\n",
      "1.17 [dog, car, house]\n",
      "1.19 [book, pen]\n",
      "1.2 [Earth, cloud]\n",
      "1.3 [chair, dog, car, house]\n",
      "1.33 [ring, table, apple, tree, flower]\n",
      "1.41 [sun, moon, sky, Earth, cloud]\n",
      "1.46 [Africa, star]\n",
      "1.49 [computer, phone, book, pen]\n"
     ]
    }
   ],
   "source": [
    "clusters = clue_generator.cluster_words(my_words)\n",
    "for cluster in clusters:\n",
    "    print(np.round(cluster.distance, 2), cluster.get_lemmas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_num = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_clues = []\n",
    "with open('filtered_clues.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        token = line.strip()\n",
    "        lemma = nlp(token)\n",
    "        if lemma.has_vector:\n",
    "            all_clues.append(lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1) create an array for the word vectors and an array for the words\n",
    "#    to speed up calculations\n",
    "all_clue_words = np.array([w.text for w in all_clues])\n",
    "all_clue_vectors = np.array([w.vector for w in all_clues])\n",
    "\n",
    "# 2) save the all_clue_vectors array to a numpy object\n",
    "np.save('all_clue_words.npy', all_clue_words)\n",
    "#np.save('all_clue_vectors.npy', all_clue_vectors)\n",
    "\n",
    "\n",
    "# 3) save the all_clue_words and all_clue_vectors arrays to a CSV file\n",
    "#df = pd.DataFrame({'words': all_clue_words, 'vectors': all_clue_vectors.tolist()})\n",
    "#df.to_csv('all_clue_words_vectors.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "possible clues: 29943\n",
      "(29943,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rc/lxb7w_kn1gd1gppq2y1hq2tm0000gn/T/ipykernel_49494/1887027835.py:24: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  possible_clues = np.array(possible_clues)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['cloud', 'sky', 'fly', 'fish'],\n",
       " array([sea, sun, air, eye, ice, ash, red, wind, jet, rain, oil, dry, snow,\n",
       "        light, water, low, gas, land, winds, waters, tree, sand, seas,\n",
       "        lake, mid, day, use, heat, fire, trees, moon, eat, birds, bird,\n",
       "        ray, prey, way, skin, eyes, egg, dust, ocean, grow, flow, end,\n",
       "        soil, tail, hot, map, web, ball, feet, rod, big, wood, edge, lot,\n",
       "        ship, boat, blue, nest, food, flight, storm, dark, river, hole,\n",
       "        thin, lakes, ink, skies, bat, plane, mud, bed, fat, sail, thing,\n",
       "        wet, wings, rains, bit, sands, cut, rock, area, space, rays, car,\n",
       "        waves, ions, reef, roof, body, rocks, eggs, dog, wing, night,\n",
       "        areas], dtype=object))"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cluster_words = ['space', 'star', 'moon']\n",
    "#cluster_words = [str(w) for w in np.random.choice(all_clue_words, 3, replace=False)]\n",
    "cluster_words = ['cloud', 'sky', 'fly', 'fish']\n",
    "my_words = cluster_words\n",
    "your_words = ['cloud', 'sky', 'fly', 'fish']\n",
    "cluster_vectors = np.array([nlp(w).vector for w in cluster_words])\n",
    "your_vectors = np.array([nlp(w).vector for w in your_words])\n",
    "\n",
    "\n",
    "# 2) find a word that is closest to the average vector of the cluster\n",
    "#    and farthest from the other words outside the cluster\n",
    "def no_word_overlap(word, words):\n",
    "    # make sure the word does not equal,\n",
    "    # or is not a substring of any of the words, and vice versa\n",
    "    return (word not in words) and (not any([word in w for w in words]) and (not any([w in word for w in words])))\n",
    "\n",
    "# filter out clues that overlap with the words\n",
    "possible_clues = []\n",
    "possible_vectors = []\n",
    "for clue in all_clues:\n",
    "    if no_word_overlap(clue.text, my_words) and no_word_overlap(clue.text, your_words):\n",
    "        possible_clues.append(clue)\n",
    "        possible_vectors.append(clue.vector)\n",
    "possible_clues = np.array(possible_clues)\n",
    "possible_vectors = np.array(possible_vectors)\n",
    "\n",
    "print('possible clues:', len(possible_clues))\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# calculate the cosine distance between each clue and the average vector\n",
    "cluster_dists = possible_vectors @ cluster_vectors.T\n",
    "#cluster_dists = possible_vectors @ np.expand_dims(np.mean(cluster_vectors, axis=0), 0).T\n",
    "your_dists = possible_vectors @ your_vectors.T\n",
    "# normalize the dists\n",
    "cluster_dists = 1 - cluster_dists / np.expand_dims(np.linalg.norm(cluster_vectors, axis=1),0) * np.expand_dims(np.linalg.norm(possible_vectors, axis=1), 1)\n",
    "your_dists = 1 - your_dists / np.expand_dims(np.linalg.norm(your_vectors, axis=1),0) * np.expand_dims(np.linalg.norm(possible_vectors, axis=1), 1)\n",
    "\n",
    "best_dists = np.sum(cluster_dists, axis=1) # - np.sum(your_dists, axis=1)\n",
    "print(best_dists.shape)\n",
    "\n",
    "clue_idxs = np.argsort(best_dists)[:100]\n",
    "#print(clue_idxs[:100])\n",
    "cluster_words, possible_clues[clue_idxs] #, best_dists[clue_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
